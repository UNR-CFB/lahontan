#!/usr/bin/python3

################################################################
# General Usage
################################################################

'''Usage:
    runPipe [options] [<command>] [<args>...]

Options:
    -h, --help
        Show this screen and exit
    --version
        Show version and exit
    --maxcpu <CPUs>
        Limits number of CPUs used by Pipeline. Default is to
        use all available CPUs
    --noconfirm
        Ignore all user prompts except JSON file creation
    --use-reference
        Use Reference data that has already been prepared.
        Put path to already prepared reference data in 
        INPUT file
        Note: Need to run Stage 1 with this argument
    -j <jsonFile>, --jsonfile <jsonFile>
        Ignores JSON Metadata file creation and uses specified
        path to JSON Metadata
    --reference-qc <pathtoReferences>
        Runs Quality Control check on Reference files
    --reference-pp <pathtoReferences>
        Pre processes Reference data
    --NUKE <input>
        Removes entire project Directory specified in input file

Available runPipe commands are:
    fc      Use featureCounts utilities
    st      Use Stringtie utilities
    ka      Use kallisto utilities
    mj      Provoke questionnaire to make a Metadata file

See 'runPipe help <command>' for more information on a specific 
command'''
VERSION = 'runPipe version 1.01\nAuthor: Alberto Nava'

################################################################
# Importations
################################################################

from docopt import docopt
from timeit import default_timer as timer
import time
import pipeClasses
import os

################################################################
# Utilities
################################################################

def makeGlobalVars(arguments):
    # Global Variables to be used in scripts
    global NOCONFIRM
    NOCONFIRM = arguments['--noconfirm']
    global JSFI # Metadata JSON file
    JSFI = arguments['--jsonfile']
    global IS_REFERENCE_PREPARED
    IS_REFERENCE_PREPARED = arguments['--use-reference']
    pipeClasses.JSFI = JSFI
    pipeClasses.NOCONFIRM = NOCONFIRM
    pipeClasses.IS_REFERENCE_PREPARED = IS_REFERENCE_PREPARED
    pipeClasses.pipeUtils.JSFI = JSFI
    pipeClasses.pipeUtils.NOCONFIRM = NOCONFIRM
    pipeClasses.pipeUtils.IS_REFERENCE_PREPARED = IS_REFERENCE_PREPARED

def referenceProcessing(arguments):
    ''' Arguments:                   
            None                     
        Returns:                     
            None                     

        Runs Alternate Analysis
    '''                              
    if arguments['--reference-qc']:
        NOCONFIRM = arguments['--noconfirm']
        pipeClasses.pipeUtils.NOCONFIRM = NOCONFIRM
        Init = arguments['--reference-qc']+'/.init'
        if not os.path.exists(Init):
            Gtf,Cdna,Genome = pipeClasses.pipeUtils.getReferenceVariables(
                                                    arguments['--reference-qc'])
            with open(Init,'w') as f:
                f.write('\n'.join([Gtf,Cdna,Genome]))
        else:
            with open(Init,'r') as f:
                Stuff = f.readlines()
            Gtf = Stuff[0].rstrip('\n')
            Cdna = Stuff[1].rstrip('\n')
            Genome = Stuff[2].rstrip('\n')
        pipeClasses.pipeUtils.qcReference(arguments['--reference-qc'],Genome)
        raise SystemExit
    if arguments['--reference-pp']:
        NOCONFIRM = arguments['--noconfirm']
        pipeClasses.pipeUtils.NOCONFIRM = NOCONFIRM
        Init = arguments['--reference-pp']+'/.init'
        if not os.path.exists(Init):
            Gtf,Cdna,Genome = pipeClasses.pipeUtils.getReferenceVariables(
                                                    arguments['--reference-pp'])
            with open(Init,'w') as f:
                f.write('\n'.join([Gtf,Cdna,Genome]))
        else:
            with open(Init,'r') as f:
                Stuff = f.readlines()
            Gtf = Stuff[0].rstrip('\n')
            Cdna = Stuff[1].rstrip('\n')
            Genome = Stuff[2].rstrip('\n')
        Basename = pipeClasses.pipeUtils.getBasename(Genome)
        pipeClasses.pipeUtils.preProcessingReference(arguments['--reference-pp'],
                                                    Cdna,Gtf,Genome,Basename)
        if arguments['--kallisto'] and pipeClasses.needToBuildKaliIndex(): #TODO check condition
            pipeClasses.buildKallistoIndex()
        raise SystemExit

def deleteAll(arguments):
    # Handling --NUKE argument
    if arguments['--NUKE']:
        if NOCONFIRM:
            PROJ = pipeClasses.Experiment(arguments['--NUKE'])
            PROJ.nukeProject()
            raise SystemExit
        else:
            PROJ = pipeClasses.Experiment(arguments['--NUKE'])
            while True:
                answer = input('Are you sure you wish to remove {}?(y/n) '.format(PROJ.Project))
                if answer == 'y':
                    PROJ.nukeProject()
                    raise SystemExit
                elif answer == 'n':
                    raise SystemExit
                else:
                    print('Please answer y or n')

def clean(arguments):
    # Handling Cleaning Arguments
    possibleCleanArguments = ['All','Reference','Data','Postprocessing']
    if arguments['--clean']:
        assert arguments['--clean'] in possibleCleanArguments, 'Invalid Cleaning Argument: Run runPipe.py -h for available arguments'
        PROJ = pipeClasses.Experiment(arguments['<input>'])
        if arguments['--clean'] != 'All':
            if arguments['--clean'] == 'Data':
                if not os.path.isdir(PROJ.Data):
                    raise SystemExit('{} does not exist'.format(PROJ.Data))
            if arguments['--clean'] == 'Reference':
                if not os.path.isdir(PROJ.Reference):
                    raise SystemExit('{} does not exist'.format(PROJ.Reference))
            if arguments['--clean'] == 'Postprocessing':
                if not os.path.isdir(PROJ.Postprocessing):
                    raise SystemExit('{} does not exist'.format(PROJ.Postprocessing))
        PROJ.clean(arguments['--clean'])
        raise SystemExit
    elif arguments['--sampleclean']:
        PROJ = pipeClasses.Experiment(arguments['<input>'])
        if not os.path.isdir(str(PROJ.Data + '/' + arguments['--sampleclean'])):
            raise SystemExit('{} does not exist'.format(str(PROJ.Data + '/' +
                                                        arguments['--sampleclean'])))
        PROJ.clean('Sample',arguments['--sampleclean'])
        raise SystemExit

def checkJSON():
    # Checking JSON file for syntax errors
    if JSFI != None:
        pipeClasses.checkJSON(JSFI)

def makeBatch(ExperimentClass, arguments): #TODO check max cpu
    ''' Arguments:
            ExperimentClass = class; experiment to run analysis on
        Returns:
            None

        If --makebatch argument given, then make batch script to be
        used with slurm, and then exit
    '''
    # Making slurm batch files if necessary
    if arguments['--batchjson']:
        pipeClasses.checkJSON(arguments['--batchjson'])
    if arguments['--makebatch']:
        preNodes,Nodes = arguments['--makebatch'].split(','),[]
        for node in preNodes:
            if not node.isdigit() or int(node) <= 0:
                raise SystemExit('Not a valid argument to --makebatch: {}'.format(
                                        arguments['--makebatch']))
            Nodes.append(int(node))
        if arguments['<command>'] == 'st':
            ExperimentClass.makeStringtieBatch(cluster=Nodes,jsonFile=arguments['--batchjson'])
        elif arguments['<command>'] == 'ka':
            ExperimentClass.makeKallistoBatch(cluster=Nodes,jsonFile=arguments['--batchjson'])
        else:
            ExperimentClass.makeBatch(cluster=Nodes,jsonFile=arguments['--batchjson'])
        raise SystemExit('Batch file successfully created:\n\t{}/pipeBatch'.format(os.getcwd()))

def checkMaxCPU(arguments):
    # Returning a maximum CPU value if given
    if arguments['--maxcpu']:
        if arguments['--maxcpu'].isdigit():
            Max = int(arguments['--maxcpu'])
            if Max <= os.cpu_count() and Max > 0:
                return Max
            else:
                raise SystemExit('--maxcpu greater than available CPUs')
        else:
            raise SystemExit('Invalid value to --maxcpu: {}'.format(
                        arguments['--maxcpu']))
    else:
        return None

def setGlobalLog(ExperimentClass):
    # Inititiating a Runtime Log global location
    global RUNTIMELOG
    RUNTIMELOG = str(ExperimentClass.Project + '/Runtime.log')
    pipeClasses.RUNTIMELOG = RUNTIMELOG

def checkdashr(ExperimentClass, arguments):
    # Running a specific sample
    if arguments['--runsample']:
        samples = arguments['--runsample'].split(',')
        possibleSamples = [str(a+1) for a in range(ExperimentClass.getNumberofSamples())]
        runsampleUsage = ('Invalid argument to --runsample: {}'.format(arguments['--runsample']) + 
                            'Possible arguments: {}'.format(str(possibleSamples)))
        if len(set(samples)) != len(samples):
            raise SystemExit(runsampleUsage)
        for sample in samples:
            if sample not in possibleSamples:
                raise SystemExit(runsampleUsage)
        return samples

def checkdashe(arguments):
    # Determining what stages to run of Pipeline
    if arguments['--execute']:
        executionStages = arguments['--execute'].split(',')
        possibleStages = ["1","2","3","4","5","A"]
        executeUsage = ('Invalid argument to --execute: {}'.format(arguments['--execute']) + 
                            'Possible arguments: {}'.format(str(possibleStages)))
        if len(set(executionStages)) != len(executionStages):
            raise SystemExit(executeUsage)
        for stage in executionStages:
            if stage not in possibleStages:
                raise SystemExit(executeUsage)
        return executionStages

def checkdashp(arguments):
    # Handling st --phase
    possiblePhases = ['a','b','c','ab','bc','abc']
    stringtieUsage = ('Invalid argument to --phase: {}'.format(arguments['--phase']) + 
                        'Possible arguments: {}'.format(str(possiblePhases)))
    if arguments['--phase'] not in possiblePhases:
        raise SystemExit(stringtieUsage)
    executionPhases = arguments['--stringtie'].split('')
    return executionPhases

def stage1FCReadyToExecute(ExperimentClass):
    return True

def stage2FCReadyToExecute(ExperimentClass):
    return True

def stage3FCReadyToExecute(ExperimentClass):
    return True

def stage4FCReadyToExecute(ExperimentClass):
    return True

def stage5FCReadyToExecute(ExperimentClass):
    return True

def stage1STReadyToExecute(ExperimentClass):
    return True

def stage2STReadyToExecute(ExperimentClass):
    return True

def stage3STReadyToExecute(ExperimentClass):
    return True

def stage4STReadyToExecute(ExperimentClass):
    return True

def stage5STReadyToExecute(ExperimentClass):
    return True

def stage1KAReadyToExecute(ExperimentClass):
    return True

def stage2KAReadyToExecute(ExperimentClass):
    return True

def stage3KAReadyToExecute(ExperimentClass):
    return True

def stage4KAReadyToExecute(ExperimentClass):
    return True

def stage5KAReadyToExecute(ExperimentClass):
    return True

################################################################
# Usage Functions
################################################################

def featureCounts(arguments):
    '''Usage:
    runPipe fc [options] <input>

Options:
    -h, --help
        Show this screen and exit
    -e <stage>, --execute <stage>
        Comma-separated list of stages to be executed.
        Possible stages include:
            1: Creating Project Structure
            2: Preparing Reference Data
            3: Running actual Pipeline
            4: Preparing for R analysis
            5: Running R analysis
            A: (1,2,3,4,5); A=all i.e. runs entire pipeline
        [default: A]
    -r <integer>, --runsample <integer>
        Runs Stage 3 of the pipeline on the sample specified
        by the integer
    --makebatch <cluster>
        Makes batch file to be used with slurm. The argument
        it takes is a comma-separated list of CPUs on each
        node in your cluster
    --batchjson <pathtoJSON>
        Uses json file already created to make batch file
        [default: False]
    --edger
        Runs edgeR analysis only. Default is to run both 
    --deseq
        Runs DESeq2 analysis only. Default is to run both
    -c <placeToClean>, --clean <placeToClean>
        Cleans <placeToClean>; Possible places include:
            Reference
            Data
            Postprocessing
            All
    --sampleclean <sampleName>
        Similar to --clean; but instead just cleans a
        single sample directory, <sampleName>'''
    makeGlobalVars(arguments)
    # Clean directories if necessary
    clean(arguments)
    # Make batch file if necessary
    makeBatch(PROJ, arguments)
    cpuLimit = checkMaxCPU(arguments)
    FCClass = pipeClasses.FCountsExperiment(arguments['<pathtoInputFile>'], maxCPU=cpuLimit)
    setGlobalLog(FCClass)
    samplesToExecute = checkdashr(FCClass, arguments)
    stagesToExecute = checkdashe(arguments)
    # Run Stage 1 if necessary
    if '1' in stagesToExecute and stage1FCReadyToExecute(FCClass):
        FCClass.runStage1()
    # Run Stage 2 if necessary
    if '2' in stagesToExecute and stage2FCReadyToExecute(FCClass):
        FCClass.runStage2()
    # Run Stage 3 if necessary
    if '3' in stagesToExecute and stage3FCReadyToExecute(FCClass):
        if samplesToExecute == None:
            FCClass.runStage3()
        else:
            for sample in samplesToExecute:
                FCClass.executeSample(sample)
    # Run Stage 4 if necessary
    if '4' in stagesToExecute and stage4FCReadyToExecute(FCClass):
        FCClass.runStage4()
    # Run Stage 5 if necessary
    if '5' in stagesToExecute and stage5FCReadyToExecute(FCClass):
        if arguments['--edger'] or arguments['--deseq']:
            if arguments['--edger']:
                FCClass.runEdgeR()
            if arguments['--deseq']:
                FCClass.runDESeq()
        else:
            FCClass.runStage5()

def stringtie(arguments):
    '''Usage:
    runPipe st [options] <input>

Options:
    -h, --help
        Show this screen and exit
    -e <stage>, --execute <stage>
        Comma-separated list of stages to be executed.
        Possible stages include:
            1: Creating Project Structure
            2: Preparing Reference Data
            3: Running actual Pipeline
            4: Preparing for R analysis
            5: Running R analysis
            A: (1,2,3,4,5); A=all i.e. runs entire pipeline
        [default: A]
    -r <integer>, --runsample <integer>
        Runs Stage 3 of the pipeline on the sample specified
        by the integer
    -p <phase>, --phase <phase>
        Use stringtie tools to replace featureCounts. phase can
        be any of: "a","b","c","ab","bc","abc"
        Option also used to specify stringtie postprocessing
        options in which case use: "--execute 4 --stringtie abc"
        Note: If you will be running phase b, you cannot specify
        a sample to run with --runsample
        [default: abc]
    --makebatch <cluster>
        Makes batch file to be used with slurm. The argument
        it takes is a comma-separated list of CPUs on each
        node in your cluster
    --batchjson <pathtoJSON>
        Uses json file already created to make batch file
        [default: False]
    -c <placeToClean>, --clean <placeToClean>
        Cleans <placeToClean>; Possible places include:
            Reference
            Data
            Postprocessing
            All
    --sampleclean <sampleName>
        Similar to --clean; but instead just cleans a
        single sample directory, <sampleName>'''
    makeGlobalVars(arguments)
    # Clean directories if necessary
    clean(arguments)
    # Make batch file if necessary
    makeBatch(PROJ, arguments)
    cpuLimit = checkMaxCPU(arguments)
    StringtieClass = pipeClasses.StringtieExperiment(arguments['<pathtoInputFile>'], maxCPU=cpuLimit)
    setGlobalLog(StringtieClass)
    samplesToExecute = checkdashr(StringtieClass, arguments)
    stagesToExecute = checkdashe(arguments)
    phasesToExecute = checkdashp(arguments)
    # Run Stage 1 if necessary
    if '1' in stagesToExecute and stage1STReadyToExecute(StringtieClass):
        StringtieClass.runStage1()
    # Run Stage 2 if necessary
    if '2' in stagesToExecute and stage2STReadyToExecute(StringtieClass):
        StringtieClass.runStage2()
    # Run Stage 3 if necessary
    if '3' in stagesToExecute and stage3STReadyToExecute(StringtieClass):
        if samplesToExecute == None:
            StringtieClass.runStage3(phasesToExecute)
        else:
            for sample in samplesToExecute:
                StringtieClass.executeSample(sample, phasesToExecute)
    # Run Stage 4 if necessary
    if '4' in stagesToExecute and stage4STReadyToExecute(StringtieClass):
        StringtieClass.runStage4()
    # Run Stage 5 if necessary
    if '5' in stagesToExecute and stage5STReadyToExecute(StringtieClass):
        StringtieClass.runStage5()

def kallisto(arguments):
    '''Usage:
    runPipe ka [options] <input>

Options:
    -h, --help
        Show this screen and exit
    -e <stage>, --execute <stage>
        Comma-separated list of stages to be executed.
        Possible stages include:
            1: Creating Project Structure
            2: Preparing Reference Data
            3: Running actual Pipeline
            4: Preparing for R analysis
            5: Running R analysis
            A: (1,2,3,4,5); A=all i.e. runs entire pipeline
        [default: A]
    -r <integer>, --runsample <integer>
        Runs Stage 3 of the pipeline on the sample specified
        by the integer
    --makebatch <cluster>
        Makes batch file to be used with slurm. The argument
        it takes is a comma-separated list of CPUs on each
        node in your cluster
    --batchjson <pathtoJSON>
        Uses json file already created to make batch file
        [default: False]
    -c <placeToClean>, --clean <placeToClean>
        Cleans <placeToClean>; Possible places include:
            Reference
            Data
            Postprocessing
            All
    --sampleclean <sampleName>
        Similar to --clean; but instead just cleans a
        single sample directory, <sampleName>'''
    makeGlobalVars(arguments)
    # Clean directories if necessary
    clean(arguments)
    # Make batch file if necessary
    makeBatch(PROJ, arguments)
    cpuLimit = checkMaxCPU(arguments)
    KallistoClass = pipeClasses.KallistoExperiment(arguments['<pathtoInputFile>'], maxCPU=cpuLimit)
    setGlobalLog(KallistoClass)
    samplesToExecute = checkdashr(KallistoClass, arguments)
    stagesToExecute = checkdashe(arguments)
    # Run Stage 1 if necessary
    if '1' in stagesToExecute and stage1KAReadyToExecute(KallistoClass):
        KallistoClass.runStage1()
    # Run Stage 2 if necessary
    if '2' in stagesToExecute and stage2KAReadyToExecute(KallistoClass):
        KallistoClass.runStage2()
    # Run Stage 3 if necessary
    if '3' in stagesToExecute and stage3KAReadyToExecute(KallistoClass):
        if samplesToExecute == None:
            KallistoClass.runStage3()
        else:
            for sample in samplesToExecute:
                KallistoClass.executeSample(sample)
    # Run Stage 4 if necessary
    if '4' in stagesToExecute and stage4KAReadyToExecute(KallistoClass):
        KallistoClass.runStage4()
    # Run Stage 5 if necessary
    if '5' in stagesToExecute and stage5KAReadyToExecute(KallistoClass):
        KallistoClass.runStage5()

def otherStuff(arguments):
    if any(arguments.values()):
        makeGlobalVars(arguments)
        deleteAll(arguments)
        referenceProcessing(arguments)
    else:
        raise SystemExit(__doc__)

def moreHelp(arguments):
    makeGlobalVars(arguments)
    try:
        helpCommand = arguments['<args>'][0]
    except IndexError:
        raise SystemExit(__doc__)
    if helpCommand == 'fc':
        print('This is featureCounts')
    elif helpCommand == 'st':
        print('This is stringtie')
    elif helpCommand == 'ka':
        print('This is kallisto')
    elif helpCommand == 'help':
        print('Help command provides help')
    else:
        raise SystemExit(__doc__)

################################################################
# Reading Command-Line Arguments
################################################################

if __name__ == '__main__':
    beginTime = timer()

    args = docopt(__doc__, version=VERSION, options_first=True)
    argv = [args['<command>']] + args['<args>']
    if args['<command>'] == 'fc':
        args.update(docopt(featureCounts.__doc__, argv=argv))
        featureCounts(args)
    elif args['<command>'] == 'st':
        args.update(docopt(stringtie.__doc__, argv=argv))
        stringtie(args)
    elif args['<command>'] == 'ka':
        args.update(docopt(kallisto.__doc__, argv=argv))
        kallisto(args)
    elif args['<command>'] == 'mj':
        import makeJSON
        args.update(docopt(makeJSON.__doc__, argv=argv))
        makeJSON.writeJSON(args['--jsonfile'])
    elif args['<command>'] == 'help':
        moreHelp(args)
    elif args['<command>'] == None:
        otherStuff(args)
    else:
        raise SystemExit("{} is not a runPipe command. See runPipe help".format(args['<command>']))

    timeused = str(time.strftime('%H:%M:%S', time.gmtime(timer() - beginTime)))
    print('Total time elapsed: {}'.format(timeused))
